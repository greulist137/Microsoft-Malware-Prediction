{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from dummyPy import OneHotEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameImputer(TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Impute missing values.\n",
    "\n",
    "        Columns of dtype object are imputed with the most frequent value \n",
    "        in column.\n",
    "\n",
    "        Columns of other types are imputed with mean of column.\n",
    "\n",
    "        \"\"\"\n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],\n",
    "            index=X.columns)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newImpute(df):\n",
    "    for col in df.columns:\n",
    "        #print(col)\n",
    "        df[col].fillna(value=df[col].value_counts(),inplace =True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainData = pd.read_csv(\"train.csv\", chunksize=1000)\n",
    "trainData = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = pd.DataFrame(trainData)\n",
    "trainData.drop('MachineIdentifier', inplace = True, axis=1)\n",
    "trainData = DataFrameImputer().fit_transform(trainData)\n",
    "trainData = newImpute(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the decimal columns to full numbers by removing the decimal points\n",
    "X.EngineVersion = X.EngineVersion.apply(lambda x: x.replace('.',''))\n",
    "X.AppVersion = X.AppVersion.apply(lambda x: x.replace('.',''))\n",
    "X.AvSigVersion = X.AvSigVersion.apply(lambda x: x.replace('.',''))\n",
    "X.OsVer = X.OsVer.apply(lambda x: x.replace('.',''))\n",
    "X.Census_OSVersion = X.Census_OSVersion.apply(lambda x: x.replace('.',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensuring all of the items are converted to Numeric calues\n",
    "X.EngineVersion = pd.to_numeric(X.EngineVersion)\n",
    "X.AppVersion = pd.to_numeric(X.AppVersion)\n",
    "X.AvSigVersion = pd.to_numeric(X.AvSigVersion)\n",
    "X.OsVer = pd.to_numeric(X.OsVer)\n",
    "X.Census_OSVersion = pd.to_numeric(X.Census_OSVersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(['ProductName', 'Platform', 'Processor', 'OsPlatformSubRelease',\n",
    "       'OsBuildLab', 'SkuEdition', 'PuaMode', 'SmartScreen',\n",
    "       'Census_MDC2FormFactor', 'Census_DeviceFamily', 'Census_ProcessorClass',\n",
    "       'Census_PrimaryDiskTypeName', 'Census_ChassisTypeName',\n",
    "       'Census_PowerPlatformRoleName', 'Census_InternalBatteryType',\n",
    "       'Census_OSArchitecture', 'Census_OSBranch', 'Census_OSEdition',\n",
    "       'Census_OSSkuName', 'Census_OSInstallTypeName',\n",
    "       'Census_OSWUAutoUpdateOptionsName', 'Census_GenuineStateName',\n",
    "       'Census_ActivationChannel', 'Census_FlightRing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting')      \n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(df)\n",
    "print('FINISHED')\n",
    "df = enc.transform(df)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Starting with Scaler')\n",
    "scaler = StandardScaler(with_mean=True)\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scree_plot(pca):\n",
    "    num_components = len(pca.explained_variance_ratio_)\n",
    "    ind = np.arange(num_components)\n",
    "    vals = pca.explained_variance_ratio_\n",
    " \n",
    "    plt.figure(figsize=(18, 6))\n",
    "    ax = plt.subplot(111)\n",
    "    cumvals = np.cumsum(vals)\n",
    "    ax.bar(ind, vals)\n",
    "    ax.plot(ind, cumvals)\n",
    "    for i in range(num_components):\n",
    "        ax.annotate(r\"%s%%\" % ((str(vals[i]*100)[:4])), (ind[i]+0.2, vals[i]), va=\"bottom\", ha=\"center\", fontsize=12)\n",
    " \n",
    "    ax.xaxis.set_tick_params(width=0)\n",
    "    ax.yaxis.set_tick_params(width=2, length=12)\n",
    " \n",
    "    ax.set_xlabel(\"Principal Component\")\n",
    "    ax.set_ylabel(\"Variance Explained (%)\")\n",
    "    plt.title('Explained Variance Per Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scree_plot(pca):\n",
    "    num_components = len(pca.explained_variance_ratio_)\n",
    "    ind = np.arange(num_components)\n",
    "    vals = pca.explained_variance_ratio_\n",
    " \n",
    "    plt.figure(figsize=(18, 6))\n",
    "    ax = plt.subplot(111)\n",
    "    cumvals = np.cumsum(vals)\n",
    "    ax.bar(ind, vals)\n",
    "    ax.plot(ind, cumvals)\n",
    "    for i in range(num_components):\n",
    "        ax.annotate(r\"%s%%\" % ((str(vals[i]*100)[:4])), (ind[i]+0.2, vals[i]), va=\"bottom\", ha=\"center\", fontsize=12)\n",
    " \n",
    "    ax.xaxis.set_tick_params(width=0)\n",
    "    ax.yaxis.set_tick_params(width=2, length=12)\n",
    " \n",
    "    ax.set_xlabel(\"Principal Component\")\n",
    "    ax.set_ylabel(\"Variance Explained (%)\")\n",
    "    plt.title('Explained Variance Per Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pca(n_components, data):\n",
    "    '''\n",
    "    Transforms data using PCA to create n_components, and provides back the results of the\n",
    "    transformation.\n",
    "\n",
    "    INPUT: n_components - int - the number of principal components to create\n",
    "           data - the data you would like to transform\n",
    "\n",
    "    OUTPUT: pca - the pca object created after fitting the data\n",
    "            X_pca - the transformed X matrix with new number of components\n",
    "    '''\n",
    "    X = StandardScaler().fit_transform(data)\n",
    "    pca = PCA(n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return pca, X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_results(full_dataset, pca):\n",
    "    '''\n",
    "    Create a DataFrame of the PCA results\n",
    "    Includes dimension feature weights and explained variance\n",
    "    Visualizes the PCA results\n",
    "    '''\n",
    "    \n",
    "    # Dimension indexing\n",
    "    dimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n",
    "\n",
    "    \n",
    "    # PCA component\n",
    "    components = pd.DataFrame(np.round(pca.components_, 4), columns = full_dataset.keys())\n",
    "    components.index = dimensions\n",
    "    \n",
    "    # PCA explained variance\n",
    "    ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n",
    "    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n",
    "    variance_ratios.index = dimensions\n",
    "    \n",
    "    # Create a bar plot visualization\n",
    "    fig, ax = plt.subplots(figsize = (14,8))\n",
    "    \n",
    "    # Plot the feature weights as a function of the components\n",
    "    components.plot(ax = ax, kind = 'bar');\n",
    "    ax.set_ylabel(\"Feature Weights\")\n",
    "    ax.set_xticklabels(dimensions, rotation=0)\n",
    "    \n",
    "    # Display the explained variance ratios\n",
    "    for i, ev in enumerate(pca.explained_variance_ratio_):\n",
    "        ax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n          %.4f\"%(ev))\n",
    "\n",
    "    # Return a concatenated DataFrame\n",
    "    return pd.concat([variance_ratios, components], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See how many columns we are dealing with before I adjust PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No parameters\n",
    "pca = PCA()\n",
    "pca.fit_transform(X)\n",
    "scree_plot(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#200 components\n",
    "pca, X_pca = do_pca(200, X)\n",
    "scree_plot(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-assigning PCA to X\n",
    "X = X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the label column (y) and then dropping it from dataframe\n",
    "#y = training_dataset['HasDetections'].values\n",
    "y = torch.tensor(training_dataset['HasDetections'].values.astype(np.float32))\n",
    "#X = torch.tensor(X.values.astype(np.float32))\n",
    "X = torch.tensor(X.astype(np.float32))\n",
    "#train_tensor = data_utils.TensorDataset(X, y) \n",
    "#training_dataset.drop('HasDetections', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set hyperparameters\n",
    "n_in = X.shape[1]\n",
    "n_h_1 = 100\n",
    "n_h_2 = 50\n",
    "n_h_3 = 25\n",
    "n_out = 1\n",
    "num_epochs=5\n",
    "batch_size = 100\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(n_in, n_h_1),\n",
    "                     nn.ReLU(),\n",
    "                    nn.Linear(n_h_1, n_h_2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_h_2, n_h_3),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_h_3,n_out),\n",
    "                     nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    # Forward Propagation\n",
    "    y_pred = model(X)\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print('epoch: ', epoch,' loss: ', loss.item())\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
