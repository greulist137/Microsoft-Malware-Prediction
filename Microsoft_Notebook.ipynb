{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from sklearn.decomposition import PCA\n",
    "from dummyPy import OneHotEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\",\n",
    "                   chunksize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder([\"Pclass\", \"Sex\", \"Embarked\"])\n",
    "encoder.fit(data)\n",
    "\n",
    "sample_data = pd.read_csv(\"titanic.csv\",\n",
    "                           usecols=[\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\"],\n",
    "                           nrows=100)\n",
    "\n",
    "encoder.transform(sample_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing of Training Dataset\n",
    "#for chunk in pd.read_csv(<filepath>, chunksize=<your_chunksize_here>)\n",
    "   # do_processing()\n",
    "   # train_algorithm()\n",
    "#training_dataset = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"train.csv\", nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['ProductName', 'Platform', 'Processor', 'OsPlatformSubRelease',\n",
    "       'OsBuildLab', 'SkuEdition', 'PuaMode', 'SmartScreen',\n",
    "       'Census_MDC2FormFactor', 'Census_DeviceFamily', 'Census_ProcessorClass',\n",
    "       'Census_PrimaryDiskTypeName', 'Census_ChassisTypeName',\n",
    "       'Census_PowerPlatformRoleName', 'Census_InternalBatteryType',\n",
    "       'Census_OSArchitecture', 'Census_OSBranch', 'Census_OSEdition',\n",
    "       'Census_OSSkuName', 'Census_OSInstallTypeName',\n",
    "       'Census_OSWUAutoUpdateOptionsName', 'Census_GenuineStateName',\n",
    "       'Census_ActivationChannel', 'Census_FlightRing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\greul\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('train.csv', usecols=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('test.csv', usecols=fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  pd.get_dummies(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =  pd.get_dummies(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-386b8665a4c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "X.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clearing out the data\n",
    "X.drop(X.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"train.csv\", nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  MachineIdentifier   ProductName EngineVersion  \\\n",
      "0  0000028988387b115f69f31a3bf04f09  win8defender   1.1.15100.1   \n",
      "1  000007535c3f730efa9ea0b7ef1bd645  win8defender   1.1.14600.4   \n",
      "2  000007905a28d863f6d0d597892cd692  win8defender   1.1.15100.1   \n",
      "3  00000b11598a75ea8ba1beea8459149f  win8defender   1.1.15100.1   \n",
      "4  000014a5f00daa18e76b81417eeb99fc  win8defender   1.1.15100.1   \n",
      "\n",
      "        AppVersion  AvSigVersion  IsBeta  RtpStateBitfield  IsSxsPassiveMode  \\\n",
      "0  4.18.1807.18075  1.273.1735.0       0               7.0                 0   \n",
      "1     4.13.17134.1    1.263.48.0       0               7.0                 0   \n",
      "2  4.18.1807.18075  1.273.1341.0       0               7.0                 0   \n",
      "3  4.18.1807.18075  1.273.1527.0       0               7.0                 0   \n",
      "4  4.18.1807.18075  1.273.1379.0       0               7.0                 0   \n",
      "\n",
      "   DefaultBrowsersIdentifier  AVProductStatesIdentifier      ...       \\\n",
      "0                        NaN                    53447.0      ...        \n",
      "1                        NaN                    53447.0      ...        \n",
      "2                        NaN                    53447.0      ...        \n",
      "3                        NaN                    53447.0      ...        \n",
      "4                        NaN                    53447.0      ...        \n",
      "\n",
      "   Census_FirmwareVersionIdentifier  Census_IsSecureBootEnabled  \\\n",
      "0                           36144.0                           0   \n",
      "1                           57858.0                           0   \n",
      "2                           52682.0                           0   \n",
      "3                           20050.0                           0   \n",
      "4                           19844.0                           0   \n",
      "\n",
      "   Census_IsWIMBootEnabled  Census_IsVirtualDevice  Census_IsTouchEnabled  \\\n",
      "0                      NaN                     0.0                      0   \n",
      "1                      NaN                     0.0                      0   \n",
      "2                      NaN                     0.0                      0   \n",
      "3                      NaN                     0.0                      0   \n",
      "4                      0.0                     0.0                      0   \n",
      "\n",
      "   Census_IsPenCapable  Census_IsAlwaysOnAlwaysConnectedCapable  Wdft_IsGamer  \\\n",
      "0                    0                                      0.0           0.0   \n",
      "1                    0                                      0.0           0.0   \n",
      "2                    0                                      0.0           0.0   \n",
      "3                    0                                      0.0           0.0   \n",
      "4                    0                                      0.0           0.0   \n",
      "\n",
      "  Wdft_RegionIdentifier HasDetections  \n",
      "0                  10.0             0  \n",
      "1                   8.0             0  \n",
      "2                   3.0             0  \n",
      "3                   3.0             1  \n",
      "4                   1.0             1  \n",
      "\n",
      "[5 rows x 83 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 83)\n"
     ]
    }
   ],
   "source": [
    "#Checking overall shape of the dataset (just the columns as the rows will be much larger)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#Checking for duplicates\n",
    "print(X.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEED TO DETERMINE WHAT TO DO WITH NAN\n",
    "X = X.fillna(X.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.select_dtypes(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looks like we have about 30 columns out of our 83 that need to be investigated further to see whether we need to convert them or to drop the columns altogether\n",
    "\n",
    "The following columns are decimals and could be convertred to full numbers:EngineVersion, AppVersion,AvSigVersion, OsVer, Census_OSVersion  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, MachineIdentifier is just a unique identifier for each row.  I will be removing this column for now in the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the decimal columns to full numbers by removing the decimal points\n",
    "X.EngineVersion = X.EngineVersion.apply(lambda x: x.replace('.',''))\n",
    "X.AppVersion = X.AppVersion.apply(lambda x: x.replace('.',''))\n",
    "X.AvSigVersion = X.AvSigVersion.apply(lambda x: x.replace('.',''))\n",
    "X.OsVer = X.OsVer.apply(lambda x: x.replace('.',''))\n",
    "X.Census_OSVersion = X.Census_OSVersion.apply(lambda x: x.replace('.',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.EngineVersion = pd.to_numeric(X.EngineVersion)\n",
    "X.AppVersion = pd.to_numeric(X.AppVersion)\n",
    "X.AvSigVersion = pd.to_numeric(X.AvSigVersion)\n",
    "X.OsVer = pd.to_numeric(X.OsVer)\n",
    "X.Census_OSVersion = pd.to_numeric(X.Census_OSVersion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.drop('MachineIdentifier', inplace = True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X))\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEED TO DETERMINE WHAT TO DO WITH NAN\n",
    "X = training_dataset.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X)\n",
    "print(type(X))\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scree_plot(pca):\n",
    "    num_components = len(pca.explained_variance_ratio_)\n",
    "    ind = np.arange(num_components)\n",
    "    vals = pca.explained_variance_ratio_\n",
    " \n",
    "    plt.figure(figsize=(18, 6))\n",
    "    ax = plt.subplot(111)\n",
    "    cumvals = np.cumsum(vals)\n",
    "    ax.bar(ind, vals)\n",
    "    ax.plot(ind, cumvals)\n",
    "    for i in range(num_components):\n",
    "        ax.annotate(r\"%s%%\" % ((str(vals[i]*100)[:4])), (ind[i]+0.2, vals[i]), va=\"bottom\", ha=\"center\", fontsize=12)\n",
    " \n",
    "    ax.xaxis.set_tick_params(width=0)\n",
    "    ax.yaxis.set_tick_params(width=2, length=12)\n",
    " \n",
    "    ax.set_xlabel(\"Principal Component\")\n",
    "    ax.set_ylabel(\"Variance Explained (%)\")\n",
    "    plt.title('Explained Variance Per Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scree_plot(pca):\n",
    "    num_components = len(pca.explained_variance_ratio_)\n",
    "    ind = np.arange(num_components)\n",
    "    vals = pca.explained_variance_ratio_\n",
    " \n",
    "    plt.figure(figsize=(18, 6))\n",
    "    ax = plt.subplot(111)\n",
    "    cumvals = np.cumsum(vals)\n",
    "    ax.bar(ind, vals)\n",
    "    ax.plot(ind, cumvals)\n",
    "    for i in range(num_components):\n",
    "        ax.annotate(r\"%s%%\" % ((str(vals[i]*100)[:4])), (ind[i]+0.2, vals[i]), va=\"bottom\", ha=\"center\", fontsize=12)\n",
    " \n",
    "    ax.xaxis.set_tick_params(width=0)\n",
    "    ax.yaxis.set_tick_params(width=2, length=12)\n",
    " \n",
    "    ax.set_xlabel(\"Principal Component\")\n",
    "    ax.set_ylabel(\"Variance Explained (%)\")\n",
    "    plt.title('Explained Variance Per Principal Component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pca(n_components, data):\n",
    "    '''\n",
    "    Transforms data using PCA to create n_components, and provides back the results of the\n",
    "    transformation.\n",
    "\n",
    "    INPUT: n_components - int - the number of principal components to create\n",
    "           data - the data you would like to transform\n",
    "\n",
    "    OUTPUT: pca - the pca object created after fitting the data\n",
    "            X_pca - the transformed X matrix with new number of components\n",
    "    '''\n",
    "    X = StandardScaler().fit_transform(data)\n",
    "    pca = PCA(n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return pca, X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_results(full_dataset, pca):\n",
    "    '''\n",
    "    Create a DataFrame of the PCA results\n",
    "    Includes dimension feature weights and explained variance\n",
    "    Visualizes the PCA results\n",
    "    '''\n",
    "    \n",
    "    # Dimension indexing\n",
    "    dimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]\n",
    "\n",
    "    \n",
    "    # PCA component\n",
    "    components = pd.DataFrame(np.round(pca.components_, 4), columns = full_dataset.keys())\n",
    "    components.index = dimensions\n",
    "    \n",
    "    # PCA explained variance\n",
    "    ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)\n",
    "    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])\n",
    "    variance_ratios.index = dimensions\n",
    "    \n",
    "    # Create a bar plot visualization\n",
    "    fig, ax = plt.subplots(figsize = (14,8))\n",
    "    \n",
    "    # Plot the feature weights as a function of the components\n",
    "    components.plot(ax = ax, kind = 'bar');\n",
    "    ax.set_ylabel(\"Feature Weights\")\n",
    "    ax.set_xticklabels(dimensions, rotation=0)\n",
    "    \n",
    "    # Display the explained variance ratios\n",
    "    for i, ev in enumerate(pca.explained_variance_ratio_):\n",
    "        ax.text(i-0.40, ax.get_ylim()[1] + 0.05, \"Explained Variance\\n          %.4f\"%(ev))\n",
    "\n",
    "    # Return a concatenated DataFrame\n",
    "    return pd.concat([variance_ratios, components], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No parameters\n",
    "pca = PCA()\n",
    "pca.fit_transform(X)\n",
    "scree_plot(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#200 components\n",
    "pca, X_pca = do_pca(200, X)\n",
    "scree_plot(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-assigning PCA to X\n",
    "X = X_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the label column (y) and then dropping it from dataframe\n",
    "#y = training_dataset['HasDetections'].values\n",
    "y = torch.tensor(training_dataset['HasDetections'].values.astype(np.float32))\n",
    "#X = torch.tensor(X.values.astype(np.float32))\n",
    "X = torch.tensor(X.astype(np.float32))\n",
    "#train_tensor = data_utils.TensorDataset(X, y) \n",
    "#training_dataset.drop('HasDetections', axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set hyperparameters\n",
    "n_in = X.shape[1]\n",
    "n_h_1 = 100\n",
    "n_h_2 = 50\n",
    "n_h_3 = 25\n",
    "n_out = 1\n",
    "num_epochs=5\n",
    "batch_size = 100\n",
    "learning_rate = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(n_in, n_h_1),\n",
    "                     nn.ReLU(),\n",
    "                    nn.Linear(n_h_1, n_h_2),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_h_2, n_h_3),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(n_h_3,n_out),\n",
    "                     nn.Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    # Forward Propagation\n",
    "    y_pred = model(X)\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    print('epoch: ', epoch,' loss: ', loss.item())\n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # perform a backward pass (backpropagation)\n",
    "    loss.backward()\n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
