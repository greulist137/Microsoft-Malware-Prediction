#Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import torch
from torchvision import datasets, transforms, models
import torch.nn as nn
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.autograd import Variable
from sklearn.decomposition import PCA

from sklearn.preprocessing import Imputer
from sklearn.base import TransformerMixin

from sklearn.preprocessing import OneHotEncoder

class DataFrameImputer(TransformerMixin):

    def __init__(self):
        """Impute missing values.

        Columns of dtype object are imputed with the most frequent value 
        in column.

        Columns of other types are imputed with mean of column.

        """
    def fit(self, X, y=None):

        self.fill = pd.Series([X[c].value_counts().index[0]
            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],
            index=X.columns)

        return self

    def transform(self, X, y=None):
        return X.fillna(self.fill)
    
def newImpute(df):
    for col in df.columns:
        #print(col)
        df[col].fillna(value=df[col].value_counts(),inplace =True)
    return df

#trainData = pd.read_csv("train.csv", chunksize=10000)
#trainData = pd.read_csv("train.csv", nrows=10000)
trainData = pd.read_csv("train.csv")
df = pd.DataFrame(trainData)

#Converting the decimal columns to full numbers by removing the decimal points
X.EngineVersion = X.EngineVersion.apply(lambda x: x.replace('.',''))
X.AppVersion = X.AppVersion.apply(lambda x: x.replace('.',''))
X.AvSigVersion = X.AvSigVersion.apply(lambda x: x.replace('.',''))
X.OsVer = X.OsVer.apply(lambda x: x.replace('.',''))
X.Census_OSVersion = X.Census_OSVersion.apply(lambda x: x.replace('.',''))
X.EngineVersion = pd.to_numeric(X.EngineVersion)
X.AppVersion = pd.to_numeric(X.AppVersion)
X.AvSigVersion = pd.to_numeric(X.AvSigVersion)
X.OsVer = pd.to_numeric(X.OsVer)
X.Census_OSVersion = pd.to_numeric(X.Census_OSVersion)

X.drop('MachineIdentifier', inplace = True, axis=1)

df = DataFrameImputer().fit_transform(df)
df = newImpute(df)

encoder = ['ProductName', 'Platform', 'Processor', 'OsPlatformSubRelease',
       'OsBuildLab', 'SkuEdition', 'PuaMode', 'SmartScreen',
       'Census_MDC2FormFactor', 'Census_DeviceFamily', 'Census_ProcessorClass',
       'Census_PrimaryDiskTypeName', 'Census_ChassisTypeName',
       'Census_PowerPlatformRoleName', 'Census_InternalBatteryType',
       'Census_OSArchitecture', 'Census_OSBranch', 'Census_OSEdition',
       'Census_OSSkuName', 'Census_OSInstallTypeName',
       'Census_OSWUAutoUpdateOptionsName', 'Census_GenuineStateName',
       'Census_ActivationChannel', 'Census_FlightRing']
            
print('Starting')      
enc = OneHotEncoder(handle_unknown='ignore')
enc.fit(df)
print('FINISHED')
df = enc.transform(df)
print(df.shape)

print('Starting with Scaler')
scaler = StandardScaler(with_mean=True)
X = scaler.fit_transform(X)
X = pd.DataFrame(X)

def scree_plot(pca):
    num_components = len(pca.explained_variance_ratio_)
    ind = np.arange(num_components)
    vals = pca.explained_variance_ratio_
 
    plt.figure(figsize=(18, 6))
    ax = plt.subplot(111)
    cumvals = np.cumsum(vals)
    ax.bar(ind, vals)
    ax.plot(ind, cumvals)
    for i in range(num_components):
        ax.annotate(r"%s%%" % ((str(vals[i]*100)[:4])), (ind[i]+0.2, vals[i]), va="bottom", ha="center", fontsize=12)
 
    ax.xaxis.set_tick_params(width=0)
    ax.yaxis.set_tick_params(width=2, length=12)
 
    ax.set_xlabel("Principal Component")
    ax.set_ylabel("Variance Explained (%)")
    plt.title('Explained Variance Per Principal Component')
    
def scree_plot(pca):
    num_components = len(pca.explained_variance_ratio_)
    ind = np.arange(num_components)
    vals = pca.explained_variance_ratio_
 
    plt.figure(figsize=(18, 6))
    ax = plt.subplot(111)
    cumvals = np.cumsum(vals)
    ax.bar(ind, vals)
    ax.plot(ind, cumvals)
    for i in range(num_components):
        ax.annotate(r"%s%%" % ((str(vals[i]*100)[:4])), (ind[i]+0.2, vals[i]), va="bottom", ha="center", fontsize=12)
 
    ax.xaxis.set_tick_params(width=0)
    ax.yaxis.set_tick_params(width=2, length=12)
 
    ax.set_xlabel("Principal Component")
    ax.set_ylabel("Variance Explained (%)")
    plt.title('Explained Variance Per Principal Component')
    
def do_pca(n_components, data):
    '''
    Transforms data using PCA to create n_components, and provides back the results of the
    transformation.

    INPUT: n_components - int - the number of principal components to create
           data - the data you would like to transform

    OUTPUT: pca - the pca object created after fitting the data
            X_pca - the transformed X matrix with new number of components
    '''
    X = StandardScaler().fit_transform(data)
    pca = PCA(n_components)
    X_pca = pca.fit_transform(X)
    return pca, X_pca


def pca_results(full_dataset, pca):
    '''
    Create a DataFrame of the PCA results
    Includes dimension feature weights and explained variance
    Visualizes the PCA results
    '''
    
    # Dimension indexing
    dimensions = dimensions = ['Dimension {}'.format(i) for i in range(1,len(pca.components_)+1)]

    
    # PCA component
    components = pd.DataFrame(np.round(pca.components_, 4), columns = full_dataset.keys())
    components.index = dimensions
    
    # PCA explained variance
    ratios = pca.explained_variance_ratio_.reshape(len(pca.components_), 1)
    variance_ratios = pd.DataFrame(np.round(ratios, 4), columns = ['Explained Variance'])
    variance_ratios.index = dimensions
    
    # Create a bar plot visualization
    fig, ax = plt.subplots(figsize = (14,8))
    
    # Plot the feature weights as a function of the components
    components.plot(ax = ax, kind = 'bar');
    ax.set_ylabel("Feature Weights")
    ax.set_xticklabels(dimensions, rotation=0)
    
    # Display the explained variance ratios
    for i, ev in enumerate(pca.explained_variance_ratio_):
        ax.text(i-0.40, ax.get_ylim()[1] + 0.05, "Explained Variance\n          %.4f"%(ev))

    # Return a concatenated DataFrame
    return pd.concat([variance_ratios, components], axis = 1)



