#Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import torch
from torchvision import datasets, transforms, models
import torch.nn as nn
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.autograd import Variable
from sklearn.decomposition import PCA

from sklearn.preprocessing import Imputer
from sklearn.base import TransformerMixin

from sklearn.preprocessing import OneHotEncoder

class DataFrameImputer(TransformerMixin):

    def __init__(self):
        """Impute missing values.

        Columns of dtype object are imputed with the most frequent value 
        in column.

        Columns of other types are imputed with mean of column.

        """
    def fit(self, X, y=None):

        self.fill = pd.Series([X[c].value_counts().index[0]
            if X[c].dtype == np.dtype('O') else X[c].mean() for c in X],
            index=X.columns)

        return self

    def transform(self, X, y=None):
        return X.fillna(self.fill)
    
def newImpute(df):
    for col in df.columns:
        #print(col)
        df[col].fillna(value=df[col].value_counts(),inplace =True)
    return df

enc = OneHotEncoder(handle_unknown='ignore')
model = None
newModel = True
criterion = None
optimizer = None
learningRate = 0.01
loss = None

def preProcessing(trainData, isInitial, newModel, model,  optimizer, loss):
    print('MODEL')
    print(model)
    temp_y = trainData['HasDetections']
    trainData.drop('HasDetections', inplace = True, axis=1)
    trainData = pd.DataFrame(trainData)
    trainData.drop('MachineIdentifier', inplace = True, axis=1)
    trainData.drop('PuaMode', inplace = True, axis=1)
    trainData = DataFrameImputer().fit_transform(trainData)
    trainData = newImpute(trainData)
    #Converting the decimal columns to full numbers by removing the decimal points
    trainData.EngineVersion = trainData.EngineVersion.apply(lambda x: x.replace('.',''))
    trainData.AppVersion = trainData.AppVersion.apply(lambda x: x.replace('.',''))
    trainData.AvSigVersion = trainData.AvSigVersion.apply(lambda x: x.replace('.',''))
    trainData.OsVer = trainData.OsVer.apply(lambda x: x.replace('.',''))
    trainData.Census_OSVersion = trainData.Census_OSVersion.apply(lambda x: x.replace('.',''))
    print('Starting')    
    #print(trainData.isnull().sum() > 0)
    #enc = OneHotEncoder(handle_unknown='ignore')
    if(isInitial):
        enc.fit(trainData)
    else:
        y = torch.tensor(temp_y.values, dtype = torch.float64)
        trainData = enc.transform(trainData)
        print('Starting with Scaler')
        scaler = StandardScaler(with_mean=False)
        trainData = scaler.fit_transform(trainData)
        #trainData = pd.DataFrame(trainData)
        print('Finished with Scaler')
        trainData = pd.DataFrame(trainData.toarray())
        trainData = trainData.as_matrix()
        X = torch.tensor(trainData, dtype=torch.float64)
        print(newModel)
        if (newModel):
            #Set hyperparameters
            n_in = X.shape[1]
            n_h_1 = 100
            n_h_2 = 50
            n_h_3 = 25
            n_out = 1
            model = nn.Sequential(nn.Linear(n_in, n_h_1),
                                  nn.ReLU(),
                                  nn.Dropout(0.5),
                                  nn.Linear(n_h_1, n_h_2),
                                  nn.ReLU(),
                                  nn.Dropout(0.5),
                                  nn.Linear(n_h_2, n_h_3),
                                  nn.ReLU(),
                                  nn.Dropout(0.5),
                                  nn.Linear(n_h_3,n_out),
                                  nn.Sigmoid())
            newModel = False
            criterion = torch.nn.MSELoss()
            optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)
        X = Variable(X).float()
        y = Variable(y).float()
        for epoch in range(50):
            if (epoch % 2 == 0):
                # Forward Propagation
                model.eval()
                y_pred = model.forward(X)
                # Compute and print loss
                loss = criterion(y_pred, y)
                # perform a backward pass (backpropagation)
                loss.backward()
                print('epoch: ', epoch,' loss: ', loss.item())
            else:
                # Forward Propagation
                model.train()
                # Zero the gradients
                optimizer.zero_grad()
                y_pred = model.forward(X)
                # Compute and print loss
                loss = criterion(y_pred, y)
                # perform a backward pass (backpropagation)
                loss.backward()
                print('epoch: ', epoch,' loss: ', loss.item())
                # Update the parameters
                optimizer.step()                
        #y=None
        return model, optimizer, loss
    print('FINISHED')
    
for df in pd.read_csv('train.csv', chunksize=20000):
    preProcessing(df, True, newModel, model, optimizer, loss)
    
for df in pd.read_csv('train.csv', chunksize=20000):
   model, optimizer, loss =  preProcessing(df, False, newModel, model, optimizer, loss)

checkpoint = {
        'state_dict': model.state_dict(),
        'arch': model,
        'epochs': 50,
        'optimizer': optimizer.state_dict(),
        'learning_rate': learningRate,
        }
torch.save(checkpoint, 'checkpoint.pth')